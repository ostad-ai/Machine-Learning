# Machine Learing and Data Science
1) **Naive Bayes classifier** for categorical data from scratch in Python 
2) **Naive Bayes classifier** for continuous data from scratch in Python 
3) **Data Visualization:** Showing Iris dataset with Blender API
4) **Norms in vector space:** A review of norms, and reminding p-norms are included. Finally, we compare some special p-norms.
5) **Inner products in vector space:** Reminding dot product and Frobenius inner product, and then canonical norms based on them. There are examples with module *numpy*.
6) **Gram-Schmidt** process: An algorithm to convert a linearly independent set of vectors into an orthogonal set of vectors.
7) **Boxplot:** The elements of a **boxplot** are reviewed here, including: medians, quartiles, fences, and outliers.
8) **Probability, standard terms:** such as sample space, trial, outcome, and event.
9) **Logisitic function:** It is an S-shaped curve, which is widely used in machine learning and neural networks.
10) **Sigmoid functions (curves):** Some examples are included. They are widely used in neural networks and deep learning.
11) **Conditional probability:** We review the conditional probability and based on it, we get the multiplication rule. 
12) **Inclusion-exclusion principle:** We review this principle both in set theory and in probability. Python code is also provided. 
13) **Probability, independent events:** The property of independent events are mentioned here. Also, multiplication rule is included with some examples.
14) **Probability, Bayes' rule:** The Bayes' rule is expressed here along the total probability theorem. Bayes' rule is defined by conditional probabilities. Some Python code are included too.
15) **Linear Regression with Least Squares**:  When we assume the data points are related through a linear function, we can predict the dependent variable from independent variabe(s). This is a lienar regression. One way to find the parameters of a linear regression is to use a Least Squares estimator. The related Python code clarifies this topic.
16) **Ridge Regression with Least Squares**: Ridge regression is an extension of linear regression in which a penalty term is included in the loss function. This penalty term is called regularization term. Ridge regression is especially useful when data points are noisy and/or having outliers. It also shows robustness against overfitting.
17) **Gradient Descent for Linear and Ridge Regression:** This time we are going to use the *Gradient Descent method* for finding the minimum of loss function of *ridge regression* and *linear regression*. For a deeper look at the Gradient Descent (GD), see our repository for *Optimization*.
18) **Gradient and tangent  planes:** For a surface in the form of f(x,y,z)=constant, its gradient vector is orthogonal to the surface at that point. With this property, we can get the equation for tangent planes to a surface or a level curve. It is reminded that a tangent plane is a *linear approximation* to the given function.
19) **Lasso regression and Elastic Net**: After becoming familar with *Ridge regression*, we should become acquainted with *Lasso* and *Elastic Net*. In Lasso, we use the *L1* norm for the regularization term. However, in Elastic Net, we employ both *L1* and *L2* norms for regularization. This post mentions Lasso and Elastic Net with an example on a real dataset for *classification* using *subgradient method*.
20) **Coordinate Descent for Lasso:** In the previous post, we talked about *Lasso regression*, which was done by *subgradient method*. In this post, however, we use the *coordinate descent* for Lasso regression. For a deeper discussion on *Coordinate Descent*, see our post in the repository of **Optimization**.
21) **Probability, Discrete Random Variables:** Discrete random variables take values from a *countable* set. When we have discrete random variables, their distribution is defined by a step function. Here, we use *Bernoulli distribution* as an example of those representing discrete random variables. Also, we compute the *entropy* of a Bernoulli distribution for different values of its parameter. 
22) **Probability, Continuous Random Variables:** Continuous random variables often take values from an interval of real numbers. In fact, when we measure things such as speed, voltage, profit, and etc; we are dealing with continuous random variables. As an example, we review the *continuous uniform distribution function* and its *probability (density) function*.  
23) **Maximum Likelihood Estimation:** When we have a number of samples taken from a probability function, which we do not know its parameters, one way to estimate the parameters is to use the *maximum likelihood estimation* (MLE). Here, we review the MLE and apply it to two examples.
24) **MLE (maximum likelihood estimation) for a multivariate normal distribution:** We have *n* data points of the same multivariate normal distribution, which are independent. The parameters of the normal distribution, *mean vector* and *covariance matrix*, are unknown. By the MLE, we develop formulae to estimate those parameters. The Python code provides an example and shows the *ellipse* of the estimated covariance matrix.
25) **Mixture distributions:** In a mixture distribution, we have a collection of individual distributions from which samples are taken based on a probability *weights*. Here, this kind of disribution is reviewed along an example with *normal* (Gaussian) distributions in Python code. 
26) **Voronoi diagram:** Voronoi diagram or **tessellation** is a *partition* of points in a *metric space* based on their closeness to some given points named **seeds**. For a seed, the region of the space for which the points are closest to the seed is called the **Voronoi cell** of the seed. In this Notebook, we express the Python codes to partition the 2D Euclidean space into Voronoi cells. Two famous *distance functions*, **Euclidean** and **Manhattan**, are mentioned with examples.
27) **The k-means clustering:** The k-means algorithm is used for *clustering* when the number of clusters is given in advance. It is a widely-used clustering algorithm. Several versions of the k-means have also been proposed. Each cluster is represented by the mean of its data points. That is why it is called the k-means. In this notebook, we implement the standard k-means algorithm from scratch in Python. And the *iris* dataset is employed for the clustering. It should be reminded that the k-means is related to *Voronoi diagram* expressed in the previous post.
28) **Gaussian Mixture Models with the EM algorithm**: A Gaussian mixture model (GMM) is a probabilistic model in which we assume the data points come from a mixture of several **Gaussian** (normal) distributions with unknown parameters. The **Expectation-Maximiation** (EM) algorithm has been used for estimating the unknown parameters of a GMM given some data points. Here, we provide the EM algorithm for the GMMs from scratch in Python with an example using the iris dataset.  
29) **Expected value:** The expected value or **expectation** is a weighted average of all possible outcomes, where the weights are the probabilities of each outcome. And it is a measure of the central tendency of a random variable. Here, we express the expected value for both *discrete* and *continuous* randam variables with examples in Python.
30) **Law of total expectation:** The law of total expectation allows us to compute the overall expectation E[X] by breaking it into smaller, conditional expectations E[X|Y=y]. This approach is particularly useful when X depends on another random variable Y, and it is easier to compute E[X|Y=y] first. Here, we use the law of total expectation when Y is a discrete random variable, and also when Y is a continuous random variable. The examples are in Python.
31) **Radial basis functions, definiton:** A radial basis function (RBF) is a radial function which positive definite (PD). This positive definitness is defined based on the **interpolation** matrix that is formed by distint points as inputs to the radial function. For cetain radial functions, the interpolation matrix is nonsingular, which we call them to be the stricly radial basis functions. Here, three radial basis functions (RBFs) are reviewed with *Python* code. Later, we will use RBFs for interpolation and **approximation**. 
32) **Radial basis functions for interpolation:** In the previous post, we reviewed RBFs (radial basis functions). In this post, we use RBFs for interpolation. When we use an RBF, which is strictly positive definite, then the iterpolation matrix is nonsingular; and thus the parameters of the interpolation function is computed in an straightforward manner. We express the formulae and the Python code of RBF for interpolation.
33) **RBF (radial basis function) networks:** This time we use RBFs for function approximation in a three-layer structure. The RBFs are placed in the hidden layer of an RBF network. Each RBF has a center point and a width. The center points are usually obtained as the cluster centers of the input data points. The widths of the RBFs are usually selected by a heuristic. The weights of the output layer are estimated by a gradient descent method or pseudo-inverse. The complete Python code of an RBF network with the bias term and regularization, which uses stochastic gradient descent (SGD), is provided here.
34) **An artificial neuron:** Any complex artificial neuroal network is mostly composed of several artificial neurons. Here, we review a model of an artificial neuron. The Python code is also provided.
35) **Multilayer perceptron for regression:** Multilayer perceptron is a *feedforward neural network* in which we often use the **backpropagation** algorithm to adjust its weights ans biases by the given sample data. 
Here, we define and implement a three-layer MLP for regression. For regression, we use linear neurons in the output layer. The neurons of the hidden layer use a nonlinear activation function. The input layer just distributes the input data into the MLP. 
We have talked about backpropagation in a separate repository. In cotnrast, we use now a matrix-form of the backpropagation and train the weights and biases of the MLP using a mini-batch of data at each step. The complete code for the MLP for regression from scratch is provided here.
36) **Variance:** The variance of a random variable measures its spread. Specifically, it is the *expected value* of the squared deviation from the *mean* of the random variable. We talked earlier about thr expected value. Here, the Python code gives examples both for discrete and continuous random variables on how to compute variance from scratch.
37) **Covariance:** The covariance between two random variables measures the linear relationship between them. If cov(X,Y)=0 for random variables X and Y, we call them *uncorrelated*. In this post, we review and implement the covariance. The Python code includes both discrete and continuous random variables. 
38) **Covariance matrix**: The covariance matrix extends the idea of variance/covariance to higher dimensions. For a column random vector **X** of size *n*, we get a symemtrix *n*-by-*n* covariance matrix such that the diagonal entries are the variances of the components of **X**; and the other entries are the covariances between components of **X**. Here, we compute the covariance matrix for a sample of data points both from scratch and with *Numpy*.
39) **Gini impurity:** The Gini impurity is a *diversity measure*, which is used to measure how diverse or impure a dataset is with respect to the class labels of its elments. Specifically, Gini Impurity measures the probability of incorrectly classifying a randomly chosen element from a dataset if its label were predicted based on the distribution of labels in the dataset. 
Here, we implement the Gini impurity in Python with some examples. It is reminded that Gini impurity is often used in decision tree learning algorithms to decide on which feature to split the dataset. 
40) **Entropy:** The entropy is another diversity measure. It is also used in decision tree learning algorithms similar to the Gini impurity mentioned before. Entropy also quantifies the average amount of uncetainty or information for a random variable.
We use the entropy here to measure the divesity of a dataset with code in Python. 
It should be reminded that the Gini impurity is faster to compute than entropy. But for categorical features, the entropy is a better choice. 
41) **Node splitting in decision trees:** A fundamental mechanism in decision trees is the **node splitting**. At each node, we decide on which feature of the dataset, the split should happen. This splitting of dataset is guided by a diversity measure such as *Gini impurity*, *informaton gain*, or *variance reduction*.
Here, we review the node splitting in decision trees and give its related Python code. The assumption is that the features are numerical and the task is the *classification*. It is reminded that we can train decision trees for *regression* too. 
42) **Decision tree for classification and regression:** In the previous post, we talked about **node splitting** in decision trees. Here, we use the node splitting to build a decision tree with the given dataset for classification or regression tasks.
Actually, at each node of a decision tree, a decision is made, which leads us to a branch of the node. We continue this process until we get to a leaf node. The leaf node represents a class label or a real value (real number). 
In this post, we implement the decision tree from scratch, which can be used for classification and regression. The assumption is that only the features must be numerical. As an example, a decision tree is built for classification with the **Iris** dataset.
43) **Single-linkage clustering:** Single-linkage clustering is an **agglomerative hierarchical** clustering that repeatedly merges the closest two clusters based on the minimum distance between any pair of points from different clusters. Essentially, **Kruskal's algorithm** for **MST** (minimum spanning tree) is the same as **single-linkage clustering** when considering a **complete graph** between data points. We have talked about kruskal's algorithm in the repository of **Graph-Analysis**.
Here, we use Kruskal's algorithm for MST to implement single-linkage clustering from scratch in Python. We test the clustering algorithm with random data points and also with **Iris** dataset. It is seen that single-linkage clustering merges overlapping clusters into one cluster, which is a drawback of this algorithm.
44) **Agglomerative hierarchical clustering, a unified framework:** Here, we introduce a unified framework for **agglomerative hierarchical clustering** (AHC). In the AHC, we consider each data point as its own cluster. Then, we repeatedly merge two closest clusters until one cluster remains. The **closeness** between clusters is defined by the type of the **linkage**. In the unified framework here, we include four linkage types: `single-linkage`,`complete-linkage`,`average-linkage`, and `Ward-linkage`.
It is reminded that we introduced single-linkage in the previous post, which was implemented by the Kruskal's algorithm.
The AHC is tested by two toy datasets. Then, the AHC is tested by the Iris dataset with four different linkage types. For comparison, we have brought the AHC code by **Scipy**.
45) **TF-IDF** (**term frequency-inverse document frequency**)**:** TF-IDF is a **statistical** measure or we may call it a **feature engineering** tool that measures word importance in a document relative to a collection of documents (called corpus). 
TF-IDF powers **search engine** results by ranking documents most relevant to a **query**. Beyond search, it is used for *text summarization*, *keyword extraction*, and as a foundational feature in machine learning tasks like **document classification** and **clustering**.
Here, we implement TF-IDF from scratch in Python. Then, we use it for a small collection of documents to score terms. As a bonus, we provide the Python code to work with **scikit-learn** for TF-IDF.
46) **Population variance versus sample variance:** Population variance measures the average squared deviation of all values in the entire population from the true **population mean**. 
In contrast, when we don't have the entire population, and instead we have a sample of it, we need to compute the **sample variance**. Sample variance estimates the population variance from a sample of data, using the **sample mean**. 
Here, we review the formulae related to the two variances, and run a simulation to validate them.
47) **Feature scaling (normalization):** Feature scaling is a **preprocessing** technique that transforms *numerical* input features to a common *scale*, ensuring that no single feature dominates others due to differences in magnitude or units. It improves the *performance* and *convergence* of many machine learning algorithms—especially those that rely on distance measures (like **KNN**, **SVM**) or gradient-based optimization (like **neural networks**). It should be reminded that **tree-based** models (e.g., **Random Forest**) typically do not require feature scaling. 
Common scaling methods include **Standard Scaler** (zero mean, unit variance), **MinMax Scaler** (fixed range, e.g., [0,1]), and **Robust Scaler** (uses median and IQR, robust to outliers). 
Here, we implement the three mentioned scaling methods from scratch in Python, and use them with two different examples.
48) **Feature selection, Variance threshold:** It is an **unsupervised filter** method that removes features whose variance across samples is below a specified **threshold**. It operates on the principle that features with very low variability (e.g., constant or near-constant values) are unlikely to contribute useful information for prediction. 
Since this method ignores the target variable, it’s often used as a fast *preprocessing step* to eliminate obviously uninformative features before applying more sophisticated, *supervised* selection techniques.
Here, we implement variance threshold from scratch in Python and test it with a simple dataset. We also provide the code to use `VarianceThreshold` of **scikit-learn**. 
49) **Feature selection, Pearson correlation:** Feature selection by Pearson correlation is a **supervised** **filter**-based feature selection. It selects **features** based on the strength of their **linear** relationship with a numerical **target** variable. It computes the **Pearson correlation** coefficient between each feature and the target, then retains features with the highest absolute correlation values. This method is only suitable for **regression** tasks (not classification) and captures only linear dependencies, potentially missing non-linear but relevant relationships.
Here, we implement feature selection by pearson correlation from scratch in Python. Then, we test it against a simple dataset. As a bonus, we bring the code to do this kind of feature selection by **scikit-learn**.
50) **Kernel density estimation:** Kernel density estimation (**KDE**) is a **non-parametric** method for estimating the **probability density function** (pdf) of a random variable. It works by placing a smooth **kernel function** (typically Gaussian) at each data point and summing them to create a continuous density estimate. The key parameter is the **bandwidth**, which controls the smoothness: small bandwidth yields a wiggly estimate, large bandwidth yields an oversmoothed estimate.
Here, we implement KDE from scratch in Python. Then, we use it for a synthetic dataset. In the code, we include a few kernel functions to choose from. Finally, we give an example of KDE by **scikit-learn**. 
51) **Joint Probability mass function:** For discrete random variables X and Y, **joint probability mass function** or **joint PMF**, denoted by p(x,y), gives the probability that X takes value x and Y takes value y, simultaneously. Here, we reveiw the joint PMF and some other related topics such as **marginal** and **conditional** PMFs. Also, we mention how to estimate the true PMF by the **empirical PMF**.
Finally, we bring a Python code to illustrate the discussed topic.  
52) **Mutual information, discrete-discrete:** Mutual information (**MI**) measures the amount of **information** that one random variable reveals about another. It quantifies how much knowing one variable reduces uncertainty about the other. MI is always **non-negative**, and it equals **zero** if and only if the variables are statistically **independent**.
Here, we review the formulae of MI for the case where the two random variables are **discrete**. Then, we implement it in Python. Finally, we test the code with a few  examples. We also give the code to compute the MI between any number of discrete random variables by using **entropies**.
53) **Two-dimensional Kernel density estimation** (**2D KDE**)**:** 2D KDE estimates the joint probability density function of **bivariate** data by placing a 2D kernel (like Gaussian) at each data point and summing them, normalized by (`n.hx.hy`). The bandwidths `hx` and `hy`​ control smoothing, and are selected by a chosen bandwidth selection rule. Earlier, we talked about KDE for one-dimensional data points. This post extends the 1D KDE to the 2D KDE. We often use both of them in Machine Learning.
In the Notebook file uploaded, we give the Python code of a detailed version of 2D KDE along a minimal version of 2D KDE, both from scratch. We then test the 2D KDE with a synthetic dataset. As a bonus, we provide the code that uses `KernelDensity` of **Scikit-learn**.
54) **Monte Carlo to estimate expectation:** The Monte Carlo method estimates an expectation by: First generating random samples from the relevant probability distribution. Then, computing the function of interest for each sample. Finally, taking the average of these computed values. As the number of samples increases, this average converges to the expected value due to the *Law of Large Numbers*.
Here, we implement Monte Carlo for expectation in two versions: **Direct sampling** and **importance sampling**. A few examples are also provided.