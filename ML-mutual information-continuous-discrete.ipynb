{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce00f",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "### Mutual information for continuous-discrete case\n",
    "**Mutual information** (**MI**) when the feature variable $X$ is continuous and the target $Y$ is discrete with support $\\mathcal{Y}=\\{y_1,y_2,...,y_K\\}$ may be defined by:\n",
    "<br>$\\large I(X;Y)=\\sum_k\\int p(x,y_k)\\cdot log(\\frac{p(x,y_k)}{p(x)\\cdot p(y_k)})\\,dx$\n",
    "<br>Where\n",
    "- $p(x,y_k)$: joint pdf/pmf (probablity density function/probability mass function)  \n",
    "- $p(x),p(y_k)$: marginal pdf of $X$ and marginal pmf of $Y$ \n",
    "- Unit: nats (if natural log) or bits (if log₂)\n",
    "\n",
    "Alternatively, we may write:\n",
    "<br>$\\large I(X;Y)=\\int \\sum_k p(x,y_k)\\cdot log(\\frac{p(x,y_k)}{p(x)\\cdot p(y_k)})\\,dx$\n",
    "<br>If we use the equality $p(x,y_k)=p(x|y_k)\\cdot p(y_k)$ in above formula we get:\n",
    "<br>$\\large I(X;Y)=\\int \\sum_k p(x|y_k)\\cdot p(y_k)\\cdot log(\\frac{p(x|y_k)}{p(x)})\\,dx$ &nbsp;&nbsp;&nbsp; (1)\n",
    "<br>**Hint:** One way to compute mutual information is to use formula (1) mentioned above.\n",
    "<hr>\n",
    "Another formula to compute mutual infromation is to use the direct formula by Monte Carlo method to estimate expectation. accordinly, we may approximate the following forumla:\n",
    "<br>$\\large I(X;Y)=\\int\\int p(x,y)\\cdot log(\\frac{p(x,y)}{p(x)\\cdot p(y)})\\,dx$\n",
    "<br> with Monte carlo estimation of expectation, which leads to:\n",
    "<br> $\\large I(X;Y)\\approx\\frac{1}{n}\\sum_{i=1}^n log(\\frac{p(x_i,y_i)}{p(x_i)\\cdot p(y_i)})$\n",
    "<br>Replacing $p(x_i,y_i)$ by its equivalent $p(x_i|y_i)p(y_i)$ leads to:\n",
    "<br>$\\large I(X;Y)\\approx \\frac{1}{n}\\sum_{i=1}^n log(\\frac{p(x_i|y_i)}{p(x_i)})$ &nbsp;&nbsp;&nbsp; (2)\n",
    "<hr>\n",
    "\n",
    "**Reminder:** $I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$\n",
    "<br> Where:\n",
    "- $H(X)=−\\int p(x)\\cdot logp(x)\\,dx$ (Differential entropy)\n",
    "- $H(Y)=−\\sum_y p(y)\\cdot log⁡p(y)$ (Shannon entropy)\n",
    "- $H(X∣Y)=\\sum_y p(y)\\cdot H(X∣Y=y)$ (Conditional entropy)\n",
    "- $H(X∣Y=y)=−\\int p(x∣y)\\cdot log⁡p(x∣y)\\,dx$\n",
    "<hr>\n",
    "\n",
    "In the following, we estimate **conditional** and **marginal** densities by 1D KDE (kernel density estimation). Then, we use formula (1) or (2) to compute **mutual information**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "https://github.com/ostad-ai/Machine-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Machine-Learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d017dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0e601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInfoContinuousDiscrete:\n",
    "    \"\"\"\n",
    "    Estimate Mutual Information between continuous X and discrete Y \n",
    "    by Monte Carlo using KDE with Scott's bandwidth rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Only Scott's rule is used\n",
    "        pass\n",
    "    \n",
    "    def _scott_bandwidth(self, X):\n",
    "        \"\"\"Scott's rule for 1D: h = σ * n^(-1/5)\"\"\"\n",
    "        return np.std(X) * len(X) ** (-1/5)\n",
    "    \n",
    "    def _kde_logpdf(self, X_train, X_test, bandwidth):\n",
    "        \"\"\"Compute log PDF using Gaussian KDE.\"\"\"\n",
    "        # Vectorized computation for efficiency\n",
    "        diff = X_test[:, None] - X_train[None, :]\n",
    "        z = diff / bandwidth\n",
    "        kernels = np.exp(-0.5 * z**2) / (bandwidth * np.sqrt(2*np.pi))\n",
    "        pdf = np.mean(kernels, axis=1)\n",
    "        return np.log(np.maximum(pdf, 1e-100))\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Estimate Mutual Information.\"\"\"\n",
    "        X = np.asarray(X).ravel()\n",
    "        Y = np.asarray(Y).ravel()\n",
    "        \n",
    "        if len(X) != len(Y):\n",
    "            raise ValueError(\"X and Y must have same length\")\n",
    "        \n",
    "        n = len(X)\n",
    "        \n",
    "        # Get classes and probabilities\n",
    "        unique_classes, class_counts = np.unique(Y, return_counts=True)\n",
    "        p_y = class_counts / n\n",
    "        \n",
    "        # Marginal bandwidth\n",
    "        h_marginal = self._scott_bandwidth(X)\n",
    "        \n",
    "        # Estimate log p(x) for all samples\n",
    "        log_px = self._kde_logpdf(X, X, h_marginal)\n",
    "        \n",
    "        # Estimate p(x|y) for each class\n",
    "        log_px_given_y = np.zeros(n)\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            mask = (Y == cls)\n",
    "            X_class = X[mask]\n",
    "            \n",
    "            if len(X_class) > 1:\n",
    "                h_class = self._scott_bandwidth(X_class)\n",
    "                log_px_given_y[mask] = self._kde_logpdf(X_class, X[mask], h_class)\n",
    "            else:\n",
    "                log_px_given_y[mask] = log_px[mask]  # Fallback\n",
    "        \n",
    "        # Compute MI directly\n",
    "        mi = np.mean(log_px_given_y - log_px)\n",
    "        self.mi_ = max(mi, 0)  # MI cannot be negative\n",
    "        \n",
    "        # Optional: Also compute entropies\n",
    "        self.H_X_ = -np.mean(log_px)\n",
    "        self.H_X_given_Y_ = np.sum([\n",
    "            p_y[i] * -np.mean(log_px_given_y[Y == cls]) \n",
    "            for i, cls in enumerate(unique_classes)\n",
    "        ])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_mutual_info(self):\n",
    "        return self.mi_\n",
    "    \n",
    "    def get_entropies(self):\n",
    "        return {'H_X': self.H_X_, 'H_X_given_Y': self.H_X_given_Y_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00877c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI by direct formula: 0.4325848741329876\n",
      "Entropies: {'H_X': 1.9346301530769638, 'H_X_given_Y': 1.5020452789439762}\n",
      "MI by entropies: I(X;Y)=H(X)-H(X|Y) -> 0.43258487413298763 \n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Create synthetic data\n",
    "X = np.concatenate([\n",
    "    np.random.normal(0, 1, 300),\n",
    "    np.random.normal(2, 1.5, 400),\n",
    "    np.random.normal(-1, 0.8, 300)\n",
    "])\n",
    "Y = np.concatenate([np.zeros(300), np.ones(400), np.full(300, 2)])\n",
    "\n",
    "# Compute MI\n",
    "estimator = MutualInfoContinuousDiscrete()\n",
    "estimator.fit(X, Y)\n",
    "entropies=estimator.get_entropies()\n",
    "print(f\"MI by direct formula: {estimator.get_mutual_info()}\")\n",
    "print(f\"Entropies: {entropies}\")\n",
    "MI_entropies=entropies['H_X']-entropies['H_X_given_Y']\n",
    "print(f'MI by entropies: I(X;Y)=H(X)-H(X|Y) -> {MI_entropies} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93cbd7",
   "metadata": {},
   "source": [
    "<hr style=\"background:lightgreen; height:3px\">\n",
    "\n",
    "# Bonus \n",
    "#### Mutual informaiton by formula (1) and integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46eee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using formula (1) to estimate Mutual Information\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import simps\n",
    "\n",
    "def kde_pdf(x, samples, bandwidth):\n",
    "    \"\"\"\n",
    "    Compute KDE PDF at points x using Gaussian kernel.\n",
    "    \n",
    "    Args:\n",
    "        x: array of points to evaluate PDF (shape: [M])\n",
    "        samples: observed data (shape: [N])\n",
    "        bandwidth: scalar bandwidth\n",
    "    \n",
    "    Returns:\n",
    "        pdf: estimated PDF at x (shape: [M])\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples)\n",
    "    x = np.asarray(x)\n",
    "    # Compute pairwise differences\n",
    "    diff = (x[:, None] - samples[None, :]) / bandwidth  # Shape: [M, N]\n",
    "    kernel_vals = norm.pdf(diff)  # Gaussian kernel\n",
    "    pdf = np.mean(kernel_vals, axis=1) / bandwidth\n",
    "    return pdf\n",
    "\n",
    "def mutual_information_integration(X, Y, bandwidth=None, n_grid=1000):\n",
    "    \"\"\"\n",
    "    Estimate I(X; Y) where X is continuous, Y is discrete by formula (1)\n",
    "    \n",
    "    Args:\n",
    "        X: continuous data (1D array, shape [N])\n",
    "        Y: discrete labels (1D array, shape [N])\n",
    "        bandwidth: float or None (if None, use Scott's rule)\n",
    "        n_grid: number of points for numerical integration\n",
    "    \n",
    "    Returns:\n",
    "        mi: estimated mutual information (scalar)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).flatten()\n",
    "    Y = np.asarray(Y).flatten()\n",
    "    N = len(X)\n",
    "    \n",
    "    # Empirical probabilities for Y\n",
    "    unique_labels, counts = np.unique(Y, return_counts=True)\n",
    "    p_y = counts / N  # p(y_k)\n",
    "    \n",
    "    # Marginal bandwidth (if not provided)\n",
    "    if bandwidth is None:\n",
    "        bandwidth_global = X.std() * N ** (-1/5)\n",
    "    \n",
    "    # Integration grid\n",
    "    x_min, x_max = X.min(), X.max()\n",
    "    x_grid = np.linspace(x_min - 3*bandwidth_global, x_max + 3*bandwidth_global, n_grid)\n",
    "    \n",
    "    # Estimate marginal p(x) with global bandwidth\n",
    "    p_x = kde_pdf(x_grid, X, bandwidth_global)\n",
    "    \n",
    "    # Avoid log(0) by clipping densities\n",
    "    eps = 1e-10\n",
    "    p_x = np.clip(p_x, eps, None)\n",
    "    \n",
    "    # Initialize integrand\n",
    "    integrand = np.zeros_like(x_grid)\n",
    "    \n",
    "    # Loop over each class\n",
    "    for k, label in enumerate(unique_labels):\n",
    "        # Get samples for this class\n",
    "        X_k = X[Y == label]\n",
    "        N_k = len(X_k)\n",
    "        \n",
    "        if N_k == 0:\n",
    "            continue\n",
    "        # DIFFERENT BANDWIDTH PER CLASS\n",
    "        if bandwidth is None:\n",
    "            bandwidth_k = X_k.std() * N_k ** (-1/5) if N_k > 1 else bandwidth_global\n",
    "        else:\n",
    "            bandwidth_k = bandwidth    \n",
    "        # Estimate p(x | y_k)\n",
    "        p_x_given_y = kde_pdf(x_grid, X_k, bandwidth_k)\n",
    "        p_x_given_y = np.clip(p_x_given_y, eps, None)\n",
    "        \n",
    "        # Compute contribution: p(y_k) * p(x|y_k) * log(p(x|y_k)/p(x))\n",
    "        log_ratio = np.log(p_x_given_y / p_x)\n",
    "        integrand += p_y[k] * p_x_given_y * log_ratio\n",
    "    \n",
    "    # Numerical integration (Simpson's rule)\n",
    "    mi = simps(integrand, x_grid)\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f295719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Results:\n",
      "MI by formula (1) (integration): 0.34188036858603205 nats\n",
      "MI by formula (2) (Monte Carlo): 0.3651783944021437 nats\n",
      "Difference: 0.0233 nats\n"
     ]
    }
   ],
   "source": [
    "# Comparing MI estimation by formulae (1) and (2)\n",
    "def compare_methods():\n",
    "    \"\"\"Compare both methods on synthetic data with known properties\"\"\"\n",
    "    \n",
    "    # Create test data: 3 classes with different distributions\n",
    "    n_samples = 1000\n",
    "    Y = np.random.choice([0, 1, 2], size=n_samples, p=[0.4, 0.35, 0.25])\n",
    "    X = np.zeros(n_samples)\n",
    "    \n",
    "    # Class 0: N(0, 1)\n",
    "    X[Y == 0] = np.random.normal(0, 1, np.sum(Y == 0))\n",
    "    # Class 1: N(2, 1.5)  \n",
    "    X[Y == 1] = np.random.normal(2, 1.5, np.sum(Y == 1))\n",
    "    # Class 2: N(-1, 0.8)\n",
    "    X[Y == 2] = np.random.normal(-1, 0.8, np.sum(Y == 2))\n",
    "    \n",
    "    # Formula (1) (integration)\n",
    "    mi_F1 = mutual_information_integration(X, Y, n_grid=2000)\n",
    "    \n",
    "    # Formula (2) (Monte Carlo)\n",
    "    mi_F2 = MutualInfoContinuousDiscrete().fit(X, Y).get_mutual_info()\n",
    "    \n",
    "    print(\"Comparison Results:\")\n",
    "    print(f\"MI by formula (1) (integration): {mi_F1} nats\")\n",
    "    print(f\"MI by formula (2) (Monte Carlo): {mi_F2} nats\")\n",
    "    print(f\"Difference: {abs(mi_F1 - mi_F2):.4f} nats\")    \n",
    "    return mi_F1, mi_F2\n",
    "\n",
    "# Run comparison\n",
    "mi_F1, mi_F2 = compare_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03199a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
