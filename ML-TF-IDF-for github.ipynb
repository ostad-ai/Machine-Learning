{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce00f",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "### TF-IDF\n",
    "**TF-IDF** is a **statistical** measure  or a **feature engineering** tool that evaluates how important a word is to a document within a collection. \n",
    "- It calculates **term frequency** (how often a word appears in a document) multiplied by **inverse document frequency** (how rare the word is across all documents). \n",
    "- Common words like \"the\" get low scores, while rare, meaningful words get high scores. \n",
    "\n",
    "TF-IDF is widely used for text search, document similarity and clustering, text classification, and keyword extraction by converting text into numerical features.\n",
    "<hr>\n",
    "\n",
    "**Term Frequency** (TF) measures local importance in a document $d$, and it is usually defined by: \n",
    "<br>$\\large TF(t,d)=\\frac{number\\; of\\; times\\; term\\; t\\; appears\\; in\\; d}{total\\; terms\\; in\\; d}$\n",
    "<br>\n",
    "<br>**Inverse Document Frequency** (IDF) measures global rarity across $N$ documents, which may be defined by: \n",
    "<br>$\\large IDF(t)=log(\\frac{N}{number\\; of\\; documents\\; containing\\; t})$ \n",
    "<br>\n",
    "<br>**TF-IDF Score** is defined by: \n",
    "<br>$\\large TF-IDF(t,d)=TF(t,d)×IDF(t)$\n",
    "- Higher TF-IDF → more discriminative term for that document. \n",
    "     \n",
    "\n",
    " \n",
    "<hr>\n",
    "\n",
    "In the following, we give the **Python** code to compute TF-IDF from scratch for a collection of documents (a corpus). We should **preprocess** documents before computing TF-IDF such as removing punctuation, tokenizing, and removing stopwords. For each document in a corpus (collection of documents), we get a feature vector of numerical values that represents the document. Each component of the feature vector of a document is the score of relevant term in the document. \n",
    "- The terms of high scores in a document may be considered as keywords for the document. \n",
    "- An example for a simple documents clearifies the concept.\n",
    "\n",
    "A a **bonus**, we express some Python code based on **scikit-learn**, which shows how to use **TF-IDF** with class **TfidfVectorizer**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "https://github.com/ostad-ai/Machine-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Machine-Learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d017dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import math\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e47c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic set of English stopwords\n",
    "STOPWORDS = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and',\n",
    "    'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "    'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\",\n",
    "    'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    "    'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here',\n",
    "    \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i',\n",
    "    \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\",\n",
    "    'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "    'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n",
    "    'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she',\n",
    "    \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than',\n",
    "    'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\",\n",
    "    'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was',\n",
    "    \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what',\n",
    "    \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who',\n",
    "    \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'\n",
    "}\n",
    "\n",
    "# # Add domain-specific stopwords\n",
    "# STOPWORDS.update({'http', 'www', 'com', 'said', 'mr', 'ms'})\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Convert to lowercase, remove punctuation, tokenize, and remove stopwords.\n",
    "    \"\"\"\n",
    "    # Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())\n",
    "    # Split into tokens\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token and token not in STOPWORDS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c92d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(documents):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF from a list of document strings.\n",
    "    \n",
    "    Returns:\n",
    "        tfidf_matrix: list of dicts {term: tfidf_score}\n",
    "        vocabulary: sorted list of all terms\n",
    "    \"\"\"\n",
    "    # Preprocess all docs\n",
    "    processed_docs = [preprocess(doc) for doc in documents]\n",
    "    N = len(processed_docs)\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocabulary = sorted(set(term for doc in processed_docs for term in doc))\n",
    "    \n",
    "    # Compute TF for each doc\n",
    "    tf = []\n",
    "    for doc in processed_docs:\n",
    "        term_count = Counter(doc)\n",
    "        total_terms = len(doc)\n",
    "        tf_doc = {term: term_count.get(term, 0) / total_terms for term in vocabulary}\n",
    "        tf.append(tf_doc)\n",
    "    \n",
    "    # Compute IDF for each term\n",
    "    idf = {}\n",
    "    for term in vocabulary:\n",
    "        doc_freq = sum(1 for doc in processed_docs if term in doc)\n",
    "        idf[term] = math.log(N / (1 + doc_freq))  # +1 for smoothing\n",
    "    \n",
    "    # Compute TF-IDF\n",
    "    tfidf_matrix = []\n",
    "    for tf_doc in tf:\n",
    "        tfidf_doc = {term: tf_doc[term] * idf[term] for term in vocabulary}\n",
    "        tfidf_matrix.append(tfidf_doc)\n",
    "    \n",
    "    return tfidf_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77aa06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['artificial', 'birds', 'blue', 'cat', 'chased', 'clear', 'common', 'computers', 'dog', 'fly', 'garden', 'helps', 'high', 'household', 'intelligence', 'language', 'learning', 'machine', 'mat', 'natural', 'pets', 'processing', 'purred', 'sat', 'sky', 'softly', 'subset', 'text', 'understand']\n",
      "\n",
      "TF-IDF Scores (non-zero only):\n",
      "\n",
      "Document 1:\n",
      "  cat: 0.0811\n",
      "  mat: 0.2197\n",
      "  purred: 0.2197\n",
      "  sat: 0.2197\n",
      "  softly: 0.2197\n",
      "\n",
      "Document 2:\n",
      "  cat: 0.1014\n",
      "  chased: 0.2747\n",
      "  dog: 0.1733\n",
      "  garden: 0.2747\n",
      "\n",
      "Document 3:\n",
      "  birds: 0.1831\n",
      "  blue: 0.1831\n",
      "  clear: 0.1831\n",
      "  fly: 0.1831\n",
      "  high: 0.1831\n",
      "  sky: 0.1831\n",
      "\n",
      "Document 4:\n",
      "  artificial: 0.2197\n",
      "  intelligence: 0.2197\n",
      "  learning: 0.2197\n",
      "  machine: 0.2197\n",
      "  subset: 0.2197\n",
      "\n",
      "Document 5:\n",
      "  computers: 0.1569\n",
      "  helps: 0.1569\n",
      "  language: 0.1569\n",
      "  natural: 0.1569\n",
      "  processing: 0.1569\n",
      "  text: 0.1569\n",
      "  understand: 0.1569\n",
      "\n",
      "Document 6:\n",
      "  cat: 0.0811\n",
      "  common: 0.2197\n",
      "  dog: 0.1386\n",
      "  household: 0.2197\n",
      "  pets: 0.2197\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "# Sample document collection\n",
    "docs = [\n",
    "    \"The cat sat on the mat and purred softly\",\n",
    "    \"The dog chased the cat through the garden\",\n",
    "    \"Birds fly high in the clear blue sky\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Natural language processing helps computers understand text\",\n",
    "    \"The cat and dog are common household pets\"\n",
    "]\n",
    "tfidf_matrix, vocab = compute_tf_idf(docs)\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\nTF-IDF Scores (non-zero only):\")\n",
    "for i, doc_tfidf in enumerate(tfidf_matrix):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    for term in vocab:\n",
    "        score = doc_tfidf[term]\n",
    "        if score > 0:\n",
    "            print(f\"  {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3dcfd",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:lightblue\">\n",
    "\n",
    "# Bonus\n",
    "### Using scikit-learn for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfbd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['artificial' 'birds' 'blue' 'cat' 'chased' 'clear' 'common' 'computers'\n",
      " 'dog' 'fly' 'garden' 'helps' 'high' 'household' 'intelligence' 'language'\n",
      " 'learning' 'machine' 'mat' 'natural' 'pets' 'processing' 'purred' 'sat'\n",
      " 'sky' 'softly' 'subset' 'text' 'understand']\n",
      "TF-IDF matrix:\n",
      " [[0.         0.         0.         0.32711256 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47249269 0.         0.         0.         0.47249269 0.47249269\n",
      "  0.         0.47249269 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.38996741 0.56328241 0.\n",
      "  0.         0.         0.46189963 0.         0.56328241 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.         0.40824829\n",
      "  0.         0.         0.         0.40824829 0.         0.\n",
      "  0.40824829 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.40824829 0.         0.         0.         0.        ]\n",
      " [0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4472136  0.         0.4472136  0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4472136  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.         0.37796447\n",
      "  0.         0.         0.         0.37796447 0.         0.\n",
      "  0.         0.37796447 0.         0.37796447 0.         0.\n",
      "  0.         0.         0.         0.37796447 0.37796447]\n",
      " [0.         0.         0.         0.3397724  0.         0.\n",
      "  0.490779   0.         0.4024458  0.         0.         0.\n",
      "  0.         0.490779   0.         0.         0.         0.\n",
      "  0.         0.         0.490779   0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Bonus, using scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"The cat sat on the mat and purred softly\",\n",
    "    \"The dog chased the cat through the garden\",\n",
    "    \"Birds fly high in the clear blue sky\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Natural language processing helps computers understand text\",\n",
    "    \"The cat and dog are common household pets\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF matrix:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f54696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEYWORD EXTRACTION ===\n",
      "Document 1 top keywords: ['softly', 'purred', 'mat']\n",
      "Document 2 top keywords: ['garden', 'chased', 'dog']\n",
      "Document 3 top keywords: ['sky', 'blue', 'clear']\n",
      "Document 4 top keywords: ['intelligence', 'artificial', 'subset']\n",
      "Document 5 top keywords: ['text', 'understand', 'computers']\n",
      "Document 6 top keywords: ['pets', 'household', 'common']\n"
     ]
    }
   ],
   "source": [
    "def extract_top_keywords(documents, vectorizer, n_keywords=3):\n",
    "    \"\"\"Extract top keywords from documents using TF-IDF\"\"\"\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # Get TF-IDF scores for this document\n",
    "        feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "        \n",
    "        # Sort by TF-IDF score\n",
    "        sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top keywords\n",
    "        top_keywords = [feature_names[idx] for idx, score in sorted_scores[:n_keywords]]\n",
    "        print(f\"Document {i+1} top keywords: {top_keywords}\")\n",
    "\n",
    "print(\"=== KEYWORD EXTRACTION ===\")\n",
    "extract_top_keywords(docs, TfidfVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d58a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
