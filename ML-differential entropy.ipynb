{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce00f",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "### Differential entropy\n",
    "For a continuous random variable $X$ with probability density function (PDF) $p(x)$, the **differential entropy** $h(X)$ is defined as:\n",
    "<br> $\\large h(X)=-\\int_{\\mathcal{X}}p(x)\\cdot log p(x)\\,dx$\n",
    "<br> Where:\n",
    "- The integral is taken over the support of $X$ denoted by $\\mathcal{X}$.\n",
    "- The logarithm is typically base $e$ (units: nats) or base $2$ (units: bits).\n",
    "- Unlike discrete entropy, differential entropy can be negative, and it is not invariant under change of variables.\n",
    "- It measures the \"spread\" or uncertainty of a continuous distribution, but does not represent the average code length (as in discrete entropy).\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Reminder 1:** Theoretical Values:\n",
    "- Gaussian distribution $N(\\mu,\\sigma^2)$ has $h(X)=\\frac{1}{2}log⁡(2\\pi e \\sigma^2)$\n",
    "- Uniform distribution $U(a,b)$ has $h(X)=log⁡(b−a)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "We may estimate differential entropy for data points with pdf (probability density function) $p(x)$:\n",
    "- **Monte carlo estimation**: $h(X)\\approx -\\frac{1}{n} \\sum_{i=1}^n logp(x_i)$, $x_i\\sim p(x)$\n",
    "- **Numerical integration**: $h(X)\\approx \\int_\\mathcal{X} f(x) dx$, $f(x)=-p(x)\\cdot logp(x)$\n",
    "\n",
    "**Hint:** For both methods mentioned above, by having $n$ data points $x_1$, $x_2$, ...,$x_n$, we estimate the density function p(x) by 1D KDE (kernel density estimation): $\\hat{p}(x)=\\frac{1}{n\\cdot h}\\sum_{i=1}^n K(\\frac{x-x_i}{h})$, where **kernel** $K$ is Gaussian: $K(u)=\\frac{1}{\\sqrt{2\\pi}}e^{-u^2/2}$, and $h$ is called the **bandwidth**.\n",
    "\n",
    "<br>**Reminder 2:** For KDE (**kernel density estimation**), you may see our previous posts.\n",
    "<hr>\n",
    "\n",
    "In the following, we estimate **differential entropy** by both methods: *Monte Carlo-based*, and *integration-based* methods.\n",
    "\n",
    "<hr>\n",
    "\n",
    "https://github.com/ostad-ai/Machine-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Machine-Learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d017dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b6685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated by MC h(X): 1.3932904433936801 nats\n",
      "Theoretical h(X): 1.4189385332046727 nats\n"
     ]
    }
   ],
   "source": [
    "def _scott_bandwidth(X):\n",
    "        \"\"\"Scott's rule for 1D: h = σ * n^(-1/5)\"\"\"\n",
    "        return np.std(X) * len(X) ** (-1/5)\n",
    "\n",
    "def _kde_logpdf(X_train, X_test, bandwidth):\n",
    "    \"\"\"PROPER stable log-PDF computation.\"\"\"\n",
    "    n_train = len(X_train)\n",
    "    const = -np.log(n_train * bandwidth * np.sqrt(2*np.pi))\n",
    "    \n",
    "    log_pdf = np.zeros(len(X_test))\n",
    "    \n",
    "    for i, x in enumerate(X_test):\n",
    "        # Compute exponents directly (no exp() yet)\n",
    "        z = (x - X_train) / bandwidth\n",
    "        exponents = -0.5 * z**2  # These can be very negative\n",
    "        \n",
    "        # Log-sum-exp trick\n",
    "        max_exp = np.max(exponents)\n",
    "        log_sum = max_exp + np.log(np.sum(np.exp(exponents - max_exp)))\n",
    "        \n",
    "        log_pdf[i] = const + log_sum\n",
    "    \n",
    "    return log_pdf\n",
    "    \n",
    "def diff_entropy_MC(data):\n",
    "    \"\"\"\n",
    "    Estimate differential entropy using Monte Carlo and KDE.    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Continuous observations\n",
    "    Returns:\n",
    "    --------\n",
    "    entropy : float\n",
    "        Differential entropy in nats\n",
    "    \"\"\"\n",
    "    data = np.asarray(data).flatten()\n",
    "    \n",
    "    # Fit KDE\n",
    "    bandwidth=_scott_bandwidth(data)\n",
    "    log_pdf = _kde_logpdf(data, data,bandwidth)\n",
    "    \n",
    "    # Estimate entropy: h = -E[log f(x)] ≈ -mean(log f(x_i))\n",
    "    return -np.mean(log_pdf)\n",
    "\n",
    "# Example\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(0, 1, 1000)\n",
    "# Standard normal distribution (theoretical h = 0.5 * log(2*pi*e) ≈ 1.4189 nats)\n",
    "samples = np.random.normal(0, 1, 10000)\n",
    "h_est_MC = diff_entropy_MC(normal_data)\n",
    "h_true = 0.5 * np.log(2 * np.pi * np.e)  # ≈ 1.4189\n",
    "\n",
    "print(f\"Estimated by MC h(X): {h_est_MC} nats\")\n",
    "print(f\"Theoretical h(X): {h_true} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e7b61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated by integration h(X): 1.4344746695229058 nats\n",
      "Theoretical h(X): 1.4189385332046727 nats\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.integrate import simps\n",
    "\n",
    "def kde_pdf(x, samples, bandwidth):\n",
    "    \"\"\"\n",
    "    Compute Gaussian KDE PDF at points x.\n",
    "    \n",
    "    Args:\n",
    "        x: array of evaluation points (shape [M])\n",
    "        samples: observed data (shape [N])\n",
    "        bandwidth: scalar bandwidth (h)\n",
    "    \n",
    "    Returns:\n",
    "        pdf: estimated PDF at x (shape [M])\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples)\n",
    "    x = np.asarray(x)\n",
    "    diff = (x[:, None] - samples[None, :]) / bandwidth\n",
    "    kernel_vals = norm.pdf(diff)  # Gaussian kernel\n",
    "    return np.mean(kernel_vals, axis=1) / bandwidth\n",
    "\n",
    "def diff_entropy_intg(samples, bandwidth=None, n_grid=1000):\n",
    "    \"\"\"\n",
    "    Estimate differential entropy from 1D samples using KDE.\n",
    "    \n",
    "    Args:\n",
    "        samples: 1D array of data points (shape [N])\n",
    "        bandwidth: float (if None, use Scott's rule)\n",
    "        n_grid: number of points for numerical integration\n",
    "    \n",
    "    Returns:\n",
    "        h: estimated differential entropy (scalar, in nats)\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples).flatten()\n",
    "    N = len(samples)\n",
    "    \n",
    "    # Bandwidth selection (Scott's rule)\n",
    "    if bandwidth is None:\n",
    "        std = samples.std()\n",
    "        if std == 0:\n",
    "            return -np.inf  # Degenerate distribution\n",
    "        bandwidth = std * N ** (-1/5)\n",
    "    \n",
    "    # Integration grid (extend beyond data range)\n",
    "    x_min, x_max = samples.min(), samples.max()\n",
    "    x_grid = np.linspace(x_min - 3*bandwidth, x_max + 3*bandwidth, n_grid)\n",
    "    \n",
    "    # Estimate PDF via KDE\n",
    "    p_x = kde_pdf(x_grid, samples, bandwidth)\n",
    "    \n",
    "    # Avoid log(0)\n",
    "    eps = 1e-15\n",
    "    p_x = np.clip(p_x, eps, None)\n",
    "    \n",
    "    # Compute integrand: -p(x) * log(p(x))\n",
    "    integrand = -p_x * np.log(p_x)\n",
    "    \n",
    "    # Numerical integration (Simpson's rule)\n",
    "    h = simps(integrand, x_grid)\n",
    "    return h\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Standard normal distribution (theoretical h = 0.5 * log(2*pi*e) ≈ 1.4189 nats)\n",
    "    samples = np.random.normal(0, 1, 10000)\n",
    "    h_est = diff_entropy_intg(samples)\n",
    "    h_true = 0.5 * np.log(2 * np.pi * np.e)  # ≈ 1.4189\n",
    "    \n",
    "    print(f\"Estimated by integration h(X): {h_est} nats\")\n",
    "    print(f\"Theoretical h(X): {h_true} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f84ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
