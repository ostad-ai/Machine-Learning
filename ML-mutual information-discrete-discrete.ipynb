{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce00f",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "### Mutual information: Discrete-discrete case\n",
    "In Machine learning, **mutual information** (**MI**) measures the amount of information obtained about the target variable $Y$ by observing a feature $X$. Specifically, for two **discrete** random variables $X$ (feature) and $Y$ (target), the mutual information is defined as:\n",
    "<br>$\\large I(X;Y)=\\sum_{x\\in \\mathcal{X}}\\sum_{y\\in \\mathcal{Y}} p(x,y) \\cdot log⁡(\\frac{p(x,y)}{p(x) \\cdot p(y)})$\n",
    "<br>Where\n",
    "- $p(x,y)$: joint probability mass function (joint PMF) \n",
    "- $p(x),p(y)$: marginal probabilities  \n",
    "- $\\mathcal{X}$ and $\\mathcal{Y}$ are support (set of possible values) of $X$ and $Y$.\n",
    "- Unit: nats (if natural log) or bits (if log₂)\n",
    "\n",
    "<hr> \n",
    "\n",
    "Some properties of **mutual informaiton** (**MI**):\n",
    "- **Symmetry:** $I(X;Y)=I(Y;X)$.\n",
    "- **Non-negativity:** $I(X;Y)\\ge 0$, with equality iff $X$ and $Y$ are independent.\n",
    "- **Upper bound:** $I(X;Y)\\le min⁡(H(X),H(Y))$.\n",
    "- **Additivity** for independent variables: If $(X_1,Y_1)$ and $(X_2,Y_2)$ are independent, then $I(X_1,X_2;Y_1,Y_2)=I(X_1;Y_1)+I(X_2;Y_2)$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Computing MI from **entropies**:\n",
    "- $I(X;Y)=H(X)+H(Y)−H(X,Y)$\n",
    "- $I(X;Y)=H(X)−H(X∣Y)=H(Y)−H(Y∣X)$\n",
    "- $I(X;Y)=H(X,Y)-H(Y|X)-H(X|Y)$\n",
    "- Generally: $I(X_1,X_2,...,X_p)=\\sum_{i=1}^p H(X_i)-H(X_1,X_2,...,X_p)$\n",
    "    - Reminder: $H(X)=-\\sum_x p(x)\\cdot log\\,p(x)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the following,\n",
    "- We compute MI for two discrete random variables. \n",
    "- A minimal code to compute MI is given.\n",
    "- The code to compute MI from three entropies are mentioned.\n",
    "- Finally, a code to compute MI between any number of random variables using entropy and `Counter` from `collections`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "https://github.com/ostad-ai/Machine-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Machine-Learning/background-knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d017dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ddc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_discrete_discrete(X, Y,base=2):\n",
    "    \"\"\"\n",
    "    MI for discrete-discrete variables using empirical probabilities\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).flatten()\n",
    "    Y = np.asarray(Y).flatten()\n",
    "    n = len(X)\n",
    "    \n",
    "    # Create contingency table\n",
    "    # Get unique values\n",
    "    x_vals = np.unique(X)\n",
    "    y_vals = np.unique(Y)\n",
    "    \n",
    "    # Joint probability p(x,y)\n",
    "    joint_counts = np.zeros((len(x_vals), len(y_vals)))\n",
    "    \n",
    "    for i, x in enumerate(x_vals):\n",
    "        for j, y in enumerate(y_vals):\n",
    "            joint_counts[i, j] = np.sum((X == x) & (Y == y))\n",
    "    \n",
    "    joint_probs = joint_counts / n\n",
    "    \n",
    "    # Marginal probabilities\n",
    "    p_x = np.sum(joint_probs, axis=1)\n",
    "    p_y = np.sum(joint_probs, axis=0)\n",
    "    \n",
    "    # Compute MI\n",
    "    mi = 0\n",
    "    for i in range(len(x_vals)):\n",
    "        for j in range(len(y_vals)):\n",
    "            if joint_probs[i, j] > 0:\n",
    "                mi += joint_probs[i, j] * np.log(joint_probs[i, j] / (p_x[i] * p_y[j]))\n",
    "    \n",
    "    # Convert to desired base and ensure non-negative\n",
    "    mi = max(mi / np.log(base), 0)\n",
    "    \n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1767492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Computing MI ===\n",
      "Perfect dependence MI: 1.5850 bits\n",
      "Complete independence MI: 0.0000 bits\n",
      "Partial dependence MI: 0.2075 bits\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "print('=== Computing MI ===')\n",
    "# Example 1: Perfect dependence\n",
    "X1 = ['A', 'B', 'C', 'A', 'B', 'C']\n",
    "Y1 = ['A', 'B', 'C', 'A', 'B', 'C']\n",
    "mi1 = mi_discrete_discrete(X1, Y1, base=2)\n",
    "print(f\"Perfect dependence MI: {mi1:.4f} bits\")\n",
    "\n",
    "# Example 2: Complete independence\n",
    "X2 = ['A', 'A', 'B', 'B']\n",
    "Y2 = ['X', 'Y', 'X', 'Y']\n",
    "mi2 =  mi_discrete_discrete(X2, Y2, base=2)\n",
    "print(f\"Complete independence MI: {mi2:.4f} bits\")\n",
    "\n",
    "# Example 3: Partial dependence\n",
    "X3 = ['A', 'A', 'A', 'B', 'B', 'C']\n",
    "Y3 = ['X', 'X', 'Y', 'X', 'Y', 'Y']\n",
    "mi3 = mi_discrete_discrete(X3, Y3, base=2)\n",
    "print(f\"Partial dependence MI: {mi3:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f06cb3",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background:lightgreen\">\n",
    "\n",
    "# A minimal code for computing MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9429dac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Computing MI with the minimal code ===\n",
      "Perfect dependence MI: 1.5850 bits\n",
      "Complete independence MI: 0.0000 bits\n",
      "Partial dependence MI: 0.2075 bits\n"
     ]
    }
   ],
   "source": [
    "# Computing MI with a minimal code\n",
    "\n",
    "def mi_minimal(X, Y, base=2):\n",
    "    \"\"\"Minimal mutual information implementation\"\"\"\n",
    "    n = len(X)\n",
    "    joint, mx, my = {}, {}, {}\n",
    "    for x, y in zip(X, Y):\n",
    "        joint[(x, y)] = joint.get((x, y), 0) + 1\n",
    "        mx[x] = mx.get(x, 0) + 1\n",
    "        my[y] = my.get(y, 0) + 1\n",
    "    \n",
    "    mi= sum((c/n) * np.log((c/n) / ((mx[x]/n)*(my[y]/n))) \n",
    "               for (x, y), c in joint.items()) / np.log(base)\n",
    "    \n",
    "    return max(mi,0)\n",
    "\n",
    "print('=== Computing MI with the minimal code ===')\n",
    "print(f\"Perfect dependence MI: {mi_minimal(X1, Y1):.4f} bits\")\n",
    "print(f\"Complete independence MI: {mi_minimal(X2, Y2):.4f} bits\")\n",
    "print(f\"Partial dependence MI: {mi_minimal(X3, Y3):.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40c5f2",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background:lightgreen\">\n",
    "\n",
    "# Using entropies to compute MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acdfc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Computing MI with Entropies ===\n",
      "Perfect dependence MI: 1.5850 bits\n",
      "Complete independence MI: 0.0000 bits\n",
      "Partial dependence: 0.2075 bits\n"
     ]
    }
   ],
   "source": [
    "# Compute three entropies to get MI\n",
    "def entropy(arr,base=2):\n",
    "        # For joint entropy, arr should be tuple of arrays\n",
    "        n=len(arr)\n",
    "        if isinstance(arr, tuple):\n",
    "            vals, counts = np.unique(np.column_stack(arr), axis=0, return_counts=True)\n",
    "            n=len(arr[0])\n",
    "        else:\n",
    "            vals, counts = np.unique(arr, return_counts=True)\n",
    "            n=len(arr)\n",
    "        p = counts / n\n",
    "        return -np.sum(p * np.log(p) / np.log(base))\n",
    "    \n",
    "def mi_entropy(X, Y, base=2):\n",
    "    \"\"\"Using MI from entropy\"\"\"\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    return entropy(X,base) + entropy(Y,base) - entropy((X, Y),base)\n",
    "\n",
    "# Example\n",
    "print('=== Computing MI with Entropies ===')\n",
    "print(f\"Perfect dependence MI: {mi_entropy(X1, Y1):.4f} bits\")\n",
    "print(f\"Complete independence MI: {mi_entropy(X2, Y2):.4f} bits\")\n",
    "print(f\"Partial dependence: {mi_entropy(X3, Y3):.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29841502",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background:lightgreen\">\n",
    "\n",
    "# Bonus\n",
    "#### Computing MI between any number of random variables (two or more)\n",
    "- For one random variable, it returns its entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0123e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Computing MI for any number of random variables ===\n",
      "MI between two variables, I(X4;Y4): 0.3113 bits\n",
      "MI between three variables I(X4;Y4;Z4): 1.3113 bits\n",
      "MI for one variable = entropy I(X4): 1.0000 bits\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "def mi(*args, base=2):\n",
    "    \"\"\"Minimal MI with variable arguments\"\"\"\n",
    "    n = len(args[0])\n",
    "    \n",
    "    # Single entropy function\n",
    "    def H(vals):\n",
    "        counts = Counter(vals)\n",
    "        p = np.array(list(counts.values())) / n\n",
    "        p = p[p > 0]  # Remove zero probabilities\n",
    "        return -np.sum(p * np.log(p)) / np.log(base)\n",
    "    \n",
    "    # Individual entropies\n",
    "    H_individual = [H(arg) for arg in args]\n",
    "    \n",
    "    # Joint entropy\n",
    "    if len(args) == 1:\n",
    "        return H_individual[0]\n",
    "    else:\n",
    "        H_joint = H(zip(*args))\n",
    "        return sum(H_individual) - H_joint\n",
    "    \n",
    "#---------------------------\n",
    "# Example\n",
    "X4 = ['A', 'B', 'A', 'B']\n",
    "Y4 = ['X', 'X', 'X', 'Y'] \n",
    "Z4 = ['1', '2', '1', '2']\n",
    "\n",
    "print('=== Computing MI for any number of random variables ===')\n",
    "print(f\"MI between two variables, I(X4;Y4): {mi(X4, Y4):.4f} bits\") \n",
    "print(f\"MI between three variables I(X4;Y4;Z4): {mi(X4, Y4,Z4):.4f} bits\")\n",
    "print(f\"MI for one variable = entropy I(X4): {mi(X4):.4f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186c9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
