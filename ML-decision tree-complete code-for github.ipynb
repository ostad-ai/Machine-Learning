{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9231bf47",
   "metadata": {},
   "source": [
    "# ML, Data Analysis\n",
    "### Machine learning: Decision tree, complete code\n",
    "The **decision tree** is a supervised model used in machine learning for both classification and regression tasks. It models decisions and their possible consequences in a tree-like structure (often binary tree). It has three kinds of nodes:\n",
    " - **Root node:** Represents the whole dataset, and like internal nodes it has a decision point.\n",
    " - **Internal nodes:** Represents a decision point based on a specific feature.\n",
    " - **Leaf nodes:** Represents the final output, a class label for *classification*, or a continuous value for *regression*.\n",
    "<hr>\n",
    "\n",
    "**Hint 1:** For *classification*, the value of a **leaf node** is the class label assigned based on the majority class of the training samples in that node. For *regression*, the value of the leaf node is often the **average value** of the samples in that node.\n",
    "<br>**Hint 2:** Each node of a decision tree except leaf nodes has some **branches**. These branches are the outcomes of the decisions, leading to further internal or leaf nodes. \n",
    "<br> **Hint 3:** A decision tree selects the best feature to split the data at each node based on a diversity measure (e.g., Gini impurity, informstion gain for classification, or mean squared error (mse) for regression). We call this process **node splitting** which was covered fully in an earlier post.\n",
    "<hr>\n",
    "\n",
    "The **decision tree algorithm** may be specified as:\n",
    "    <br>1) Start at the root node with the full dataset. \n",
    "    <br>2) For each feature, evaluate all possible splits:\n",
    "        <br>&nbsp; &nbsp; - *Classification*: Minimize weighted Gini impurity or entropy.\n",
    "        $Gini=1−\\sum_i {p_i}^2$ (or $Entropy=−\\sum_i p_i log p_i$).\n",
    "        <br>&nbsp; &nbsp; &nbsp; For example, the Gini impurity after the split becomes:\n",
    "        <br> &nbsp; &nbsp; $Gini_{split}=\\frac{∣X_{left}∣}{∣X∣}\\cdot Gini(X_{left})+\\frac{∣X_{right}∣}{∣X∣}\\cdot Gini(X_{right})$\n",
    "        <br>&nbsp; &nbsp; - *Regression*: Minimize weighted MSE (equivalent to variance).\n",
    "        <br> &nbsp; &nbsp; $MSE_{split}=\\frac{∣X_{left}∣}{∣X∣}\\cdot Var(X_{left})+\\frac{∣X_{right}∣}{∣X∣}\\cdot Var(X_{right})$\n",
    "    <br>3) Select the best split (feature + threshold) that maximizes purity gain.\n",
    "    <br>4) Partition data into left/right child nodes.\n",
    "    <br>5) Repeat recursively until stopping:\n",
    "        - All samples in a node belong to the same class (classification) or have the same value (regression).\n",
    "        - Maximum depth reached.\n",
    "        - Minimum samples per node.\n",
    "       <br>6) Leaf nodes predict:\n",
    "        <br> &nbsp; - Classification: Majority class.\n",
    "        <br> &nbsp; - Regression: Mean/median of samples.\n",
    "<hr>\n",
    "\n",
    "**Hint 4:** In the formulae, the symbol $|\\cdot|$ returns the number of elements of the given set.\n",
    "<hr>\n",
    "In the following, we use train a decision tree with Iris dataset for classifiction task. We use the Gini impurity for splitting the dataset at nodes\n",
    "\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Machine-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/machine-learning/algorithms-with-python-codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a164d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36575b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tree node class\n",
    "class TreeNode:\n",
    "    def __init__(self, feature_idx=None, threshold=None,\n",
    "                      left=None, right=None, value=None):\n",
    "        self.feature_idx = feature_idx  # Index of feature to split on\n",
    "        self.threshold = threshold      # Threshold value for the split\n",
    "        self.left = left                # Left subtree (≤ threshold)\n",
    "        self.right = right              # Right subtree (> threshold)\n",
    "        self.value = value              # Class label (for leaf nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602c6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decision tree class\n",
    "# with maximum depth, and minimum samples to split a node\n",
    "# The task can be chosen to be classification or regression\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, task=\"classification\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.task = task  # \"classification\" or \"regression\"\n",
    "        self.root = None\n",
    "    \n",
    "    # Train (build) the tree with training data pairs\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "    \n",
    "    # build the tree with maximum depth and min. samples\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Stopping conditions\n",
    "        if (depth >= self.max_depth or \n",
    "            len(y) < self.min_samples_split or \n",
    "            len(set(y)) == 1):\n",
    "            return self._create_leaf_node(y)\n",
    "        \n",
    "        # Find best split\n",
    "        feature_idx, threshold = self._find_best_split(X, y)\n",
    "        if feature_idx is None:\n",
    "            return self._create_leaf_node(y)\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # Recursively build subtrees\n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        return TreeNode(feature_idx, threshold, left_subtree, right_subtree)\n",
    "    \n",
    "    # Create a leaf node with a value of instance TreeNode\n",
    "    def _create_leaf_node(self, y):\n",
    "        if self.task == \"classification\":\n",
    "            counts = Counter(y)\n",
    "            # Most frequent class\n",
    "            value = counts.most_common(1)[0][0]\n",
    "        else:  # regression\n",
    "            value = np.mean(y)\n",
    "        return TreeNode(value=value)\n",
    "\n",
    "    # Find the best (binary) split for the given data pairs\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_feature, best_threshold = None, None\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if self.task == \"classification\":\n",
    "                    gini = self._weighted_gini(y[left_mask], y[right_mask])\n",
    "                else:  # regression\n",
    "                    gini = self._weighted_mse(y[left_mask], y[right_mask])\n",
    "                \n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # Weighted Gini impurity\n",
    "    def _weighted_gini(self, y_left, y_right): # for classification\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        n_total = n_left + n_right\n",
    "        \n",
    "        gini_left = 1 - sum((np.sum(y_left == c) / n_left) ** 2 for c in set(y_left))\n",
    "        gini_right = 1 - sum((np.sum(y_right == c) / n_right) ** 2 for c in set(y_right))\n",
    "        \n",
    "        return (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
    "    \n",
    "    # Weighted variance\n",
    "    def _weighted_mse(self, y_left, y_right): # for regression\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        n_total = n_left + n_right\n",
    "        \n",
    "        mse_left = np.mean((y_left - np.mean(y_left)) ** 2)\n",
    "        mse_right = np.mean((y_right - np.mean(y_right)) ** 2)\n",
    "        \n",
    "        return (n_left / n_total) * mse_left + (n_right / n_total) * mse_right\n",
    "\n",
    "    # Predict labels for given data matrix X\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    # Predict label for single data point x\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None: # node is a leaf\n",
    "            return node.value\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)\n",
    "\n",
    "    # Print the structure of the tree\n",
    "    def print_tree(self, node=None, indent=0):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        prefix = \"    \" * indent\n",
    "        if node.value is not None:\n",
    "            print(f\"{prefix}\\u25cb Leaf: {node.value}\")\n",
    "        else:\n",
    "            print(f\"{prefix}\\u25c9 Feature_{node.feature_idx} ≤ {node.threshold}\")\n",
    "            if hasattr(node, \"left\"):\n",
    "                print(f\"{prefix}\\u27a9 True:\")\n",
    "                self.print_tree(node.left, indent + 1)\n",
    "            if hasattr(node, \"right\"):\n",
    "                print(f\"{prefix}\\u27a9 False:\")\n",
    "                self.print_tree(node.right, indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a84f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset into a Pandas dataframe\n",
    "df=pd.read_csv('https://raw.githubusercontent.com/ostad-ai/Machine-Learning/refs/heads/main/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e44c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal Length  Sepal Width  Petal Length  Petal Width        Class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0045ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Extract data matrix (X) and labels (y)\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train & predict\n",
    "tree = DecisionTree(max_depth=3,task=\"classification\")\n",
    "tree.fit(X_train, y_train)\n",
    "predictions = tree.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfceb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "◉ Feature_2 ≤ 1.9\n",
      "➩ True:\n",
      "    ○ Leaf: Iris-setosa\n",
      "➩ False:\n",
      "    ◉ Feature_2 ≤ 4.7\n",
      "    ➩ True:\n",
      "        ◉ Feature_3 ≤ 1.6\n",
      "        ➩ True:\n",
      "            ○ Leaf: Iris-versicolor\n",
      "        ➩ False:\n",
      "            ○ Leaf: Iris-virginica\n",
      "    ➩ False:\n",
      "        ◉ Feature_3 ≤ 1.7\n",
      "        ➩ True:\n",
      "            ○ Leaf: Iris-virginica\n",
      "        ➩ False:\n",
      "            ○ Leaf: Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of the learnt decision tree\n",
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be57ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
