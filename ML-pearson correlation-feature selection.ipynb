{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fce00f",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "### Pearson correlation for feature selection\n",
    "**Pearson correlation** measures the *strength* and *direction* of the **linear** relationship between a numerical feature $x$ and a numerical target $y$.\n",
    "For a feature vector $\\boldsymbol{x}$ and a target vector $\\boldsymbol{y}$, which include $n$ samples each, the **Pearson corrleation** is computed by:\n",
    "<br>$\\large r=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})}\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})}}$\n",
    "<br> Where:\n",
    "- $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$ is the **sample mean** of $\\boldsymbol{x}$.\n",
    "- $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i$ is the **sample mean** of $\\boldsymbol{y}$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Reminder 1:** If we view $\\boldsymbol{x}$  as random variable $X$, and $\\boldsymbol{y}$ as random variable $Y$; then, **Pearson correlation** may be written as:\n",
    "<br>$\\large r=\\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$\n",
    "<br> Where:\n",
    "- $Cov(X,Y)$ is the **covariance** between $X$ and $Y$.\n",
    "- $\\sigma_X$ is the **standard deviation** of $X$.\n",
    "- $\\sigma_Y$ is the **standard deviation** of $Y$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "For **feature selection** by **Pearson correlation**:\n",
    "- Compute $∣r∣$ for each feature versus target.\n",
    "- Rank features by $∣r∣$ (higher = more linearly predictive).\n",
    "- Select top $k$ features, or those above a threshold.\n",
    "\n",
    "**Hint 1:** Some points on feature selection by pearson correlation:\n",
    "- It captures only linear relationships (or sometimes monotonic ones).\n",
    "- It is sensitive to outliers.\n",
    "- It fails on non-monotonic patterns (e.g., $y=x^2$)\n",
    "- It is only for regression problems with numerical features/target, and not for classification ones\n",
    "\n",
    "**Hint 2**: A near-zero Pearson correlation does not mean \"no relationship\".\n",
    "- It just means no linear (or sometimes monotonic) relationship. Non-monotonic dependencies can still be strong\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Reminder 2:** **Monotonic Relationship** (for contrast):\n",
    "- Monotonically increasing: As $x$ increases, $y$ never decreases (may stay flat or go up).\n",
    "- Monotonically decreasing: As $x$ increases, $y$ never increases.\n",
    "- Examples: $y=x$, $y=log⁡(x)$, and $y=e^x$.\n",
    "\n",
    "**Reminder 3:** **Non-monotonic** patterns describe relationships between two variables where the direction of change is not consistently increasing or decreasing. \n",
    "- Examples:\n",
    "    - $y=x^2$ → decreases for $x<0$, increases for $x>0$.\n",
    "    - $y=sin⁡(x)$ → oscillates up and down.\n",
    "<hr>\n",
    "\n",
    "In the following, \n",
    "- We first implement the function `pearson_correlation` to compute **Pearson correlation** for given dataset X, and target set y.\n",
    "    - X is a matrix of shape `(number-of-samples, number-of-features)` such that each row is a data point.\n",
    "- We also implement the function `select_features_by_pearson` \n",
    "    - to select features based on the top `k` features with ighest correlaitons, \n",
    "    - or select all features with thier correlations higher than a threshold. \n",
    "- Finally, we test the feature selection by a simple data.\n",
    "- As a bonus, we use functions of **scikit-learn** to use feature selection by Pearson correlations.\n",
    "\n",
    "<hr>\n",
    "\n",
    "https://github.com/ostad-ai/Machine-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Machine-Learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d017dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955386d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(X, y):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient between each feature in X and target y.\n",
    "    \n",
    "    Parameters:\n",
    "        X : ndarray of shape (n_samples, n_features) — numerical features\n",
    "        y : ndarray of shape (n_samples,) — numerical target\n",
    "    \n",
    "    Returns:\n",
    "        correlations : ndarray of shape (n_features,) — Pearson r for each feature\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    \n",
    "    y = np.asarray(y).flatten()\n",
    "    \n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X and y must have the same number of samples.\")\n",
    "    \n",
    "    # Center the variables (subtract mean)\n",
    "    X_centered = X - np.mean(X, axis=0,keepdims=True)\n",
    "    y_centered = y - np.mean(y)\n",
    "    \n",
    "    # Compute covariance (numerator)\n",
    "    cov_xy = np.sum(X_centered * y_centered[:, np.newaxis], axis=0)\n",
    "    \n",
    "    # Compute standard deviations (denominator)\n",
    "    std_x = np.sqrt(np.sum(X_centered ** 2, axis=0))\n",
    "    std_y = np.sqrt(np.sum(y_centered ** 2))\n",
    "    \n",
    "    # Safe division: set correlation to 0 where denominator is 0\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        correlations = cov_xy / (std_x * std_y)\n",
    "    \n",
    "    # Replace NaN (0/0) and inf (non-zero/0) with 0\n",
    "    correlations=np.nan_to_num(correlations, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74ad455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_by_pearson(X, y, k=None, threshold=None):\n",
    "    \"\"\"\n",
    "    Select top-k or threshold-based features using Pearson correlation.\n",
    "    \n",
    "    Parameters:\n",
    "        X : feature matrix\n",
    "        y : target vector\n",
    "        k : int, optional — number of top features to select\n",
    "        threshold : float, optional — minimum |correlation| to keep\n",
    "    \n",
    "    Returns:\n",
    "        selected_X : selected features\n",
    "        selected_indices : indices of selected features\n",
    "        scores : correlation scores (absolute values)\n",
    "    \"\"\"\n",
    "    corr = pearson_correlation(X, y)\n",
    "    scores = np.abs(corr)\n",
    "    \n",
    "    if k is not None:\n",
    "        selected_indices = np.argsort(scores)[-k:]  # top-k\n",
    "    elif threshold is not None:\n",
    "        selected_indices = np.where(scores >= threshold)[0]\n",
    "    else:\n",
    "        raise ValueError(\"Either k or threshold must be specified.\")\n",
    "    \n",
    "    return X[:, selected_indices], selected_indices, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a111274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlations: [0.99965927 0.99965927 0.19626949]\n",
      "Selected feature indices: [0 1]\n",
      "New Dataset:\n",
      " [[ 1. 10.]\n",
      " [ 2. 20.]\n",
      " [ 3. 30.]\n",
      " [ 4. 40.]\n",
      " [ 5. 50.]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Sample regression data\n",
    "X = np.array([\n",
    "    [1, 10, 5],\n",
    "    [2, 20, 3],\n",
    "    [3, 30, 8],\n",
    "    [4, 40, 2],\n",
    "    [5, 50, 7]\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([2.1, 4.0, 6.2, 8.1, 10.0])  # roughly y ≈ 2*x1\n",
    "\n",
    "# Compute correlations\n",
    "corr = pearson_correlation(X, y)\n",
    "print(\"Pearson correlations:\", corr)\n",
    "# feature 2 (index 2) is weakly related\n",
    "\n",
    "# Select top k=2 features\n",
    "X_selected, idx, scores = select_features_by_pearson(X, y, k=2)\n",
    "print(\"Selected feature indices:\", idx) \n",
    "print(\"New Dataset:\\n\",X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860bb8d",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:lightblue\">\n",
    "\n",
    "# Bonus\n",
    "#### Feature selection by Pearson correlation with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11e4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset with Scikit-learn:\n",
      " [[ 1. 10.]\n",
      " [ 2. 20.]\n",
      " [ 3. 30.]\n",
      " [ 4. 40.]\n",
      " [ 5. 50.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Note: f_regression uses F-stat, but ranking ≡ |Pearson| for single features\n",
    "selector = SelectKBest(score_func=f_regression, k=2)\n",
    "X_selected_SL = selector.fit_transform(X, y)\n",
    "print(\"New Dataset with Scikit-learn:\\n\",X_selected_SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34920ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
